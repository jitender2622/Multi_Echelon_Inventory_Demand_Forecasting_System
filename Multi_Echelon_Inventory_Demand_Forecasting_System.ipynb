{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjjrJ6qmRGeW8TWF9Fkagg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jitender2622/Multi_Echelon_Inventory_Demand_Forecasting_System/blob/main/Multi_Echelon_Inventory_Demand_Forecasting_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nEPlLV-whGS",
        "outputId": "bbc720f8-5d51-4485-ee5c-8216bf2c76bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Industrial Dataset (This may take a moment)...\n",
            "✔ holiday_data.csv created.\n",
            "✔ weather_data.csv created.\n",
            "✔ historical_sales_sap.csv created with 31200 records.\n",
            "DATA GENERATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_RECORDS = 100000\n",
        "START_DATE = datetime(2016, 1, 1)\n",
        "END_DATE = datetime(2019, 6, 1) # Data up to project hand-off\n",
        "STORES = [101, 102, 103, 104, 105] # Pilot region stores\n",
        "SKUS = ['AUTO_OIL_5W30', 'SNACK_CHIPS_LG', 'BEV_ENERGY_DRINK', 'AUTO_WIPER_BLADE', 'FRESH_SANDWICH_TURKEY']\n",
        "\n",
        "print(\"Generating Industrial Dataset (This may take a moment)...\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def generate_dates(start, end):\n",
        "    delta = end - start\n",
        "    days = delta.days + 1\n",
        "    return [start + timedelta(days=i) for i in range(days)]\n",
        "\n",
        "dates = generate_dates(START_DATE, END_DATE)\n",
        "\n",
        "# --- 1. Generate Holiday Data (External API Simulation) ---\n",
        "holidays_data = {\n",
        "    'Date': [],\n",
        "    'Is_Holiday': [],\n",
        "    'Holiday_Name': []\n",
        "}\n",
        "us_holidays = {\n",
        "    (1, 1): \"New Year's Day\",\n",
        "    (7, 4): \"Independence Day\",\n",
        "    (12, 25): \"Christmas\",\n",
        "    (11, 24): \"Thanksgiving\" # Approximation for simulation\n",
        "}\n",
        "\n",
        "for d in dates:\n",
        "    holidays_data['Date'].append(d)\n",
        "    if (d.month, d.day) in us_holidays:\n",
        "        holidays_data['Is_Holiday'].append(1)\n",
        "        holidays_data['Holiday_Name'].append(us_holidays[(d.month, d.day)])\n",
        "    else:\n",
        "        holidays_data['Is_Holiday'].append(0)\n",
        "        holidays_data['Holiday_Name'].append(\"None\")\n",
        "\n",
        "df_holidays = pd.DataFrame(holidays_data)\n",
        "df_holidays.to_csv('holiday_data.csv', index=False)\n",
        "print(\"✔ holiday_data.csv created.\")\n",
        "\n",
        "# --- 2. Generate Weather Data (External API Simulation) ---\n",
        "weather_data = {\n",
        "    'Date': [],\n",
        "    'Store_ID': [],\n",
        "    'Temperature_F': [],\n",
        "    'Precipitation_In': []\n",
        "}\n",
        "\n",
        "for d in dates:\n",
        "    for store in STORES:\n",
        "        weather_data['Date'].append(d)\n",
        "        weather_data['Store_ID'].append(store)\n",
        "        # Simulate seasonality: Summer is hot, Winter is cold\n",
        "        base_temp = 75 if d.month in [6, 7, 8] else 40\n",
        "        temp = np.random.normal(base_temp, 10)\n",
        "        weather_data['Temperature_F'].append(round(temp, 1))\n",
        "        # Rain simulation\n",
        "        precip = np.random.exponential(0.1) if random.random() > 0.7 else 0.0\n",
        "        weather_data['Precipitation_In'].append(round(precip, 2))\n",
        "\n",
        "df_weather = pd.DataFrame(weather_data)\n",
        "df_weather.to_csv('weather_data.csv', index=False)\n",
        "print(\"✔ weather_data.csv created.\")\n",
        "\n",
        "# --- 3. Generate Historical Sales Data (SAP System Dump) ---\n",
        "# We will create a base dataset and then introduce \"missing data\" to simulate the SAP issues\n",
        "sales_data = []\n",
        "\n",
        "for d in dates:\n",
        "    for store in STORES:\n",
        "        for sku in SKUS:\n",
        "            # Base demand\n",
        "            base_demand = 20\n",
        "\n",
        "            # Seasonality & Holiday Spikes\n",
        "            if d.month in [6, 7, 8] and 'BEV' in sku: base_demand += 30 # Summer drinks\n",
        "            if (d.month, d.day) in us_holidays: base_demand += 50 # Holiday traffic\n",
        "            if 'AUTO' in sku and d.month in [11, 12, 1]: base_demand += 15 # Winter auto parts\n",
        "\n",
        "            # Randomness\n",
        "            noise = np.random.randint(-5, 15)\n",
        "            qty = max(0, base_demand + noise)\n",
        "\n",
        "            # Introduce Missing Data (The \"Problem\" in the project description)\n",
        "            # 5% chance the SAP system failed to record data that day\n",
        "            if random.random() < 0.05:\n",
        "                qty = np.nan\n",
        "\n",
        "            sales_data.append([d, store, sku, qty])\n",
        "\n",
        "df_sales = pd.DataFrame(sales_data, columns=['Date', 'Store_ID', 'SKU_ID', 'Qty_Sold'])\n",
        "df_sales.to_csv('historical_sales_sap.csv', index=False)\n",
        "print(f\"✔ historical_sales_sap.csv created with {len(df_sales)} records.\")\n",
        "print(\"DATA GENERATION COMPLETE.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATA INGESTION\n",
        "# ==========================================\n",
        "print(\"--- Loading Data ---\")\n",
        "df_sales = pd.read_csv('historical_sales_sap.csv', parse_dates=['Date'])\n",
        "df_weather = pd.read_csv('weather_data.csv', parse_dates=['Date'])\n",
        "df_holidays = pd.read_csv('holiday_data.csv', parse_dates=['Date'])\n",
        "\n",
        "# Merge Data (The \"Data Lake\" approach)\n",
        "# Left join sales with weather (on Date+Store) and holidays (on Date)\n",
        "df_master = df_sales.merge(df_weather, on=['Date', 'Store_ID'], how='left')\n",
        "df_master = df_master.merge(df_holidays[['Date', 'Is_Holiday']], on='Date', how='left')\n",
        "\n",
        "# Sort for Time Series processing\n",
        "df_master = df_master.sort_values(by=['Store_ID', 'SKU_ID', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA CLEANING (Linear Interpolation)\n",
        "# ==========================================\n",
        "print(\"--- Cleaning Data (Interpolation) ---\")\n",
        "# Project Requirement: Fix missing data using linear interpolation\n",
        "# We group by Store and SKU to ensure we don't interpolate across different products\n",
        "df_master['Qty_Sold'] = df_master.groupby(['Store_ID', 'SKU_ID'])['Qty_Sold'].transform(\n",
        "    lambda x: x.interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 3. FEATURE ENGINEERING\n",
        "# ==========================================\n",
        "print(\"--- Feature Engineering ---\")\n",
        "\n",
        "# Feature 1: Lags (Past sales)\n",
        "# We use 7-day lag because travel patterns are weekly\n",
        "df_master['Lag_7'] = df_master.groupby(['Store_ID', 'SKU_ID'])['Qty_Sold'].shift(7)\n",
        "\n",
        "# Feature 2: Rolling Means (Trends)\n",
        "df_master['Rolling_Mean_7'] = df_master.groupby(['Store_ID', 'SKU_ID'])['Qty_Sold'].transform(lambda x: x.rolling(window=7).mean())\n",
        "df_master['Rolling_Mean_30'] = df_master.groupby(['Store_ID', 'SKU_ID'])['Qty_Sold'].transform(lambda x: x.rolling(window=30).mean())\n",
        "\n",
        "# Feature 3: Rolling Standard Deviation (Volatility)\n",
        "# Project Requirement: Identify unstable products\n",
        "df_master['Rolling_Std_7'] = df_master.groupby(['Store_ID', 'SKU_ID'])['Qty_Sold'].transform(lambda x: x.rolling(window=7).std())\n",
        "\n",
        "# Feature 4: Date Parts\n",
        "df_master['DayOfWeek'] = df_master['Date'].dt.dayofweek\n",
        "df_master['Month'] = df_master['Date'].dt.month\n",
        "df_master['Is_Weekend'] = df_master['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# Drop rows with NaNs created by lagging/rolling (the first 30 days of data)\n",
        "df_model_data = df_master.dropna().reset_index(drop=True)\n",
        "\n",
        "print(f\"Final Dataset Shape: {df_model_data.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. MODELING (Random Forest)\n",
        "# ==========================================\n",
        "print(\"--- Training Random Forest Regressor ---\")\n",
        "\n",
        "# Define Features and Target\n",
        "features = ['Lag_7', 'Rolling_Mean_7', 'Rolling_Mean_30', 'Rolling_Std_7',\n",
        "            'Temperature_F', 'Precipitation_In', 'Is_Holiday', 'DayOfWeek', 'Month']\n",
        "target = 'Qty_Sold'\n",
        "\n",
        "# Time-Based Split (Crucial for Forecasting)\n",
        "# We train on data < 2019, Test on 2019 data (Simulating the timeline)\n",
        "split_date = pd.to_datetime('2019-01-01')\n",
        "\n",
        "train = df_model_data[df_model_data['Date'] < split_date]\n",
        "test = df_model_data[df_model_data['Date'] >= split_date]\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_test = test[features]\n",
        "y_test = test[target]\n",
        "\n",
        "# Initialize Random Forest (Parameters tuned for 2018 tech)\n",
        "rf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "predictions = rf_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"Model Performance - MAE: {mae:.2f}\")\n",
        "print(\"Note: An MAE of ~5-8 is acceptable given the high volatility of retail travel stops.\")\n",
        "\n",
        "# Feature Importance Visualization (Optional Project requirement to explain 'Why')\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"\\nTop 3 Important Features:\")\n",
        "for f in range(3):\n",
        "    print(f\"{f+1}. {features[indices[f]]} ({importances[indices[f]]:.4f})\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. FORECASTING (Next 14 Days)\n",
        "# ==========================================\n",
        "print(\"--- Generating 14-Day Forecast for SAP ---\")\n",
        "\n",
        "# We need to simulate the \"Next 14 Days\" based on the last available data point\n",
        "last_date = df_model_data['Date'].max()\n",
        "forecast_horizon = 14\n",
        "future_dates = [last_date + timedelta(days=x) for x in range(1, forecast_horizon + 1)]\n",
        "\n",
        "# Create a placeholder dataframe for future predictions\n",
        "future_rows = []\n",
        "\n",
        "# For simplicity in this demo, we will take the LAST known values of the rolling features\n",
        "# and propagate them (In production, we would re-calculate rolling recursively)\n",
        "last_known_data = df_model_data.groupby(['Store_ID', 'SKU_ID']).tail(1)\n",
        "\n",
        "for d in future_dates:\n",
        "    temp_df = last_known_data.copy()\n",
        "    temp_df['Date'] = d\n",
        "    temp_df['DayOfWeek'] = d.dayofweek\n",
        "    temp_df['Month'] = d.month\n",
        "\n",
        "    # Simulate future weather (fetching from API in real life)\n",
        "    temp_df['Temperature_F'] = 75 # Summer assumption\n",
        "    temp_df['Precipitation_In'] = 0.0\n",
        "\n",
        "    # Simulate future holiday\n",
        "    temp_df['Is_Holiday'] = 1 if (d.month, d.day) in us_holidays else 0\n",
        "\n",
        "    future_rows.append(temp_df)\n",
        "\n",
        "df_future = pd.concat(future_rows)\n",
        "\n",
        "# Predict\n",
        "df_future['Predicted_Demand'] = rf_model.predict(df_future[features])\n",
        "\n",
        "# Round up because you can't sell 1.5 units\n",
        "df_future['Predicted_Demand'] = np.ceil(df_future['Predicted_Demand']).astype(int)\n",
        "\n",
        "# ==========================================\n",
        "# 6. OUTPUT GENERATION (SAP INTEGRATION)\n",
        "# ==========================================\n",
        "# Format specifically for SAP ingestion\n",
        "sap_output = df_future[['Date', 'Store_ID', 'SKU_ID', 'Predicted_Demand']].copy()\n",
        "\n",
        "# Add logic for Safety Stock (Project Requirement: reduce stockouts)\n",
        "# If volatility (Rolling_Std) was high, add buffer\n",
        "sap_output['Safety_Buffer'] = df_future['Rolling_Std_7'].apply(lambda x: int(x * 1.5) if x > 5 else 0)\n",
        "sap_output['Final_Order_Qty'] = sap_output['Predicted_Demand'] + sap_output['Safety_Buffer']\n",
        "\n",
        "sap_output.to_csv('final_sap_orders.csv', index=False)\n",
        "print(\"✔ final_sap_orders.csv created.\")\n",
        "print(\"System Run Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "515Adx85wrmg",
        "outputId": "621bc22b-abe2-45b4-80a2-4dcb704922f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "--- Cleaning Data (Interpolation) ---\n",
            "--- Feature Engineering ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2937821705.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  lambda x: x.interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Dataset Shape: (30475, 14)\n",
            "--- Training Random Forest Regressor ---\n",
            "Model Performance - MAE: 4.48\n",
            "Note: An MAE of ~5-8 is acceptable given the high volatility of retail travel stops.\n",
            "\n",
            "Top 3 Important Features:\n",
            "1. Rolling_Mean_7 (0.6108)\n",
            "2. Is_Holiday (0.1933)\n",
            "3. Rolling_Std_7 (0.0648)\n",
            "--- Generating 14-Day Forecast for SAP ---\n",
            "✔ final_sap_orders.csv created.\n",
            "System Run Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "# Use an official Python runtime as a parent image (Version from 2019)\n",
        "FROM python:3.7-slim\n",
        "\n",
        "# Set the working directory to /app\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed packages specified in requirements.txt\n",
        "RUN pip install --trusted-host pypi.python.org -r requirements.txt\n",
        "\n",
        "# Make port 80 available to the world outside this container\n",
        "EXPOSE 80\n",
        "\n",
        "# Run model_pipeline.py when the container launches\n",
        "CMD [\"python\", \"model_pipeline.py\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRwKhRKWxin0",
        "outputId": "a70ca429-e280-4025-b201-d0af507ed000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1SBDb_vx_xK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}